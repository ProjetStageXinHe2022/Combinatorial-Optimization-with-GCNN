{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88fbca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 instances in data/instances/setcover/train_20r_30c_0.2d\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_1.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_2.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_3.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_4.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_5.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_6.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_7.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_8.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_9.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_10.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_11.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_12.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_13.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_14.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_15.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_16.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_17.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_18.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_19.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_20.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_21.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_22.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_23.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_24.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_25.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_26.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_27.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_28.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_29.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_30.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_31.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_32.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_33.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_34.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_35.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_36.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_37.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_38.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_39.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_40.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_41.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_42.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_43.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_44.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_45.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_46.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_47.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_48.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_49.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_50.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_51.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_52.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_53.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_54.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_55.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_56.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_57.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_58.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_59.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_60.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_61.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_62.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_63.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_64.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_65.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_66.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_67.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_68.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_69.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_70.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_71.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_72.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_73.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_74.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_75.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_76.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_77.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_78.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_79.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_80.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_81.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_82.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_83.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_84.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_85.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_86.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_87.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_88.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_89.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_90.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_91.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_92.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_93.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_94.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_95.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_96.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_97.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_98.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_99.lp ...\n",
      "  generating file data/instances/setcover/train_20r_30c_0.2d/instance_100.lp ...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    \"\"\"\n",
    "    Container for a graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    number_of_nodes : int\n",
    "        The number of nodes in the graph.\n",
    "    edges : set of tuples (int, int)\n",
    "        The edges of the graph, where the integers refer to the nodes.\n",
    "    degrees : numpy array of integers\n",
    "        The degrees of the nodes in the graph.\n",
    "    neighbors : dictionary of type {int: set of ints}\n",
    "        The neighbors of each node in the graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, number_of_nodes, edges, degrees, neighbors):\n",
    "        self.number_of_nodes = number_of_nodes\n",
    "        self.edges = edges\n",
    "        self.degrees = degrees\n",
    "        self.neighbors = neighbors\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The number of nodes in the graph.\n",
    "        \"\"\"\n",
    "        return self.number_of_nodes\n",
    "\n",
    "    def greedy_clique_partition(self):\n",
    "        \"\"\"\n",
    "        Partition the graph into cliques using a greedy algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of sets\n",
    "            The resulting clique partition.\n",
    "        \"\"\"\n",
    "        cliques = []\n",
    "        leftover_nodes = (-self.degrees).argsort().tolist()\n",
    "\n",
    "        while leftover_nodes:\n",
    "            clique_center, leftover_nodes = leftover_nodes[0], leftover_nodes[1:]\n",
    "            clique = {clique_center}\n",
    "            neighbors = self.neighbors[clique_center].intersection(leftover_nodes)\n",
    "            densest_neighbors = sorted(neighbors, key=lambda x: -self.degrees[x])\n",
    "            for neighbor in densest_neighbors:\n",
    "                # Can you add it to the clique, and maintain cliqueness?\n",
    "                if all([neighbor in self.neighbors[clique_node] for clique_node in clique]):\n",
    "                    clique.add(neighbor)\n",
    "            cliques.append(clique)\n",
    "            leftover_nodes = [node for node in leftover_nodes if node not in clique]\n",
    "\n",
    "        return cliques\n",
    "\n",
    "    @staticmethod\n",
    "    def erdos_renyi(number_of_nodes, edge_probability, random):\n",
    "        \"\"\"\n",
    "        Generate an Erdös-Rényi random graph with a given edge probability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        number_of_nodes : int\n",
    "            The number of nodes in the graph.\n",
    "        edge_probability : float in [0,1]\n",
    "            The probability of generating each edge.\n",
    "        random : numpy.random.RandomState\n",
    "            A random number generator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Graph\n",
    "            The generated graph.\n",
    "        \"\"\"\n",
    "        edges = set()\n",
    "        degrees = np.zeros(number_of_nodes, dtype=int)\n",
    "        neighbors = {node: set() for node in range(number_of_nodes)}\n",
    "        for edge in combinations(np.arange(number_of_nodes), 2):\n",
    "            if random.uniform() < edge_probability:\n",
    "                edges.add(edge)\n",
    "                degrees[edge[0]] += 1\n",
    "                degrees[edge[1]] += 1\n",
    "                neighbors[edge[0]].add(edge[1])\n",
    "                neighbors[edge[1]].add(edge[0])\n",
    "        graph = Graph(number_of_nodes, edges, degrees, neighbors)\n",
    "        return graph\n",
    "\n",
    "    @staticmethod\n",
    "    def barabasi_albert(number_of_nodes, affinity, random):\n",
    "        \"\"\"\n",
    "        Generate a Barabási-Albert random graph with a given edge probability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        number_of_nodes : int\n",
    "            The number of nodes in the graph.\n",
    "        affinity : integer >= 1\n",
    "            The number of nodes each new node will be attached to, in the sampling scheme.\n",
    "        random : numpy.random.RandomState\n",
    "            A random number generator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Graph\n",
    "            The generated graph.\n",
    "        \"\"\"\n",
    "        assert affinity >= 1 and affinity < number_of_nodes\n",
    "\n",
    "        edges = set()\n",
    "        degrees = np.zeros(number_of_nodes, dtype=int)\n",
    "        neighbors = {node: set() for node in range(number_of_nodes)}\n",
    "        for new_node in range(affinity, number_of_nodes):\n",
    "            # first node is connected to all previous ones (star-shape)\n",
    "            if new_node == affinity:\n",
    "                neighborhood = np.arange(new_node)\n",
    "            # remaining nodes are picked stochastically\n",
    "            else:\n",
    "                neighbor_prob = degrees[:new_node] / (2*len(edges))\n",
    "                neighborhood = random.choice(new_node, affinity, replace=False, p=neighbor_prob)\n",
    "            for node in neighborhood:\n",
    "                edges.add((node, new_node))\n",
    "                degrees[node] += 1\n",
    "                degrees[new_node] += 1\n",
    "                neighbors[node].add(new_node)\n",
    "                neighbors[new_node].add(node)\n",
    "\n",
    "        graph = Graph(number_of_nodes, edges, degrees, neighbors)\n",
    "        return graph\n",
    "\n",
    "\n",
    "def generate_setcover(nrows, ncols, density, filename, rng, max_coef=100):\n",
    "    \"\"\"\n",
    "    Generates a setcover instance with specified characteristics, and writes\n",
    "    it to a file in the LP format.\n",
    "\n",
    "    Approach described in:\n",
    "    E.Balas and A.Ho, Set covering algorithms using cutting planes, heuristics,\n",
    "    and subgradient optimization: A computational study, Mathematical\n",
    "    Programming, 12 (1980), 37-60.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nrows : int\n",
    "        Desired number of rows\n",
    "    ncols : int\n",
    "        Desired number of columns\n",
    "    density: float between 0 (excluded) and 1 (included)\n",
    "        Desired density of the constraint matrix\n",
    "    filename: str\n",
    "        File to which the LP will be written\n",
    "    rng: numpy.random.RandomState\n",
    "        Random number generator\n",
    "    max_coef: int\n",
    "        Maximum objective coefficient (>=1)\n",
    "    \"\"\"\n",
    "    nnzrs = int(nrows * ncols * density)\n",
    "\n",
    "    assert nnzrs >= nrows  # at least 1 col per row\n",
    "    assert nnzrs >= 2 * ncols  # at leats 2 rows per col\n",
    "\n",
    "    # compute number of rows per column\n",
    "    indices = rng.choice(ncols, size=nnzrs)  # random column indexes\n",
    "    indices[:2 * ncols] = np.repeat(np.arange(ncols), 2)  # force at leats 2 rows per col\n",
    "    _, col_nrows = np.unique(indices, return_counts=True)\n",
    "\n",
    "    # for each column, sample random rows\n",
    "    indices[:nrows] = rng.permutation(nrows) # force at least 1 column per row\n",
    "    i = 0\n",
    "    indptr = [0]\n",
    "    for n in col_nrows:\n",
    "\n",
    "        # empty column, fill with random rows\n",
    "        if i >= nrows:\n",
    "            indices[i:i+n] = rng.choice(nrows, size=n, replace=False)\n",
    "\n",
    "        # partially filled column, complete with random rows among remaining ones\n",
    "        elif i + n > nrows:\n",
    "            remaining_rows = np.setdiff1d(np.arange(nrows), indices[i:nrows], assume_unique=True)\n",
    "            indices[nrows:i+n] = rng.choice(remaining_rows, size=i+n-nrows, replace=False)\n",
    "\n",
    "        i += n\n",
    "        indptr.append(i)\n",
    "\n",
    "    # objective coefficients\n",
    "    c = rng.randint(max_coef, size=ncols) + 1\n",
    "\n",
    "    # sparce CSC to sparse CSR matrix\n",
    "    A = scipy.sparse.csc_matrix(\n",
    "            (np.ones(len(indices), dtype=int), indices, indptr),\n",
    "            shape=(nrows, ncols)).tocsr()\n",
    "    indices = A.indices\n",
    "    indptr = A.indptr\n",
    "\n",
    "    # write problem\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"minimize\\nOBJ:\")\n",
    "        file.write(\"\".join([f\" +{c[j]} x{j+1}\" for j in range(ncols)]))\n",
    "\n",
    "        file.write(\"\\n\\nsubject to\\n\")\n",
    "        for i in range(nrows):\n",
    "            row_cols_str = \"\".join([f\" +1 x{j+1}\" for j in indices[indptr[i]:indptr[i+1]]])\n",
    "            file.write(f\"C{i}:\" + row_cols_str + f\" >= 1\\n\")\n",
    "\n",
    "        file.write(\"\\nbinary\\n\")\n",
    "        file.write(\"\".join([f\" x{j+1}\" for j in range(ncols)]))\n",
    "\n",
    "\n",
    "\n",
    "def generate_capacited_facility_location(random, filename, n_customers, n_facilities, ratio):\n",
    "    \"\"\"\n",
    "    Generate a Capacited Facility Location problem following\n",
    "        Cornuejols G, Sridharan R, Thizy J-M (1991)\n",
    "        A Comparison of Heuristics and Relaxations for the Capacitated Plant Location Problem.\n",
    "        European Journal of Operations Research 50:280-297.\n",
    "\n",
    "    Saves it as a CPLEX LP file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random : numpy.random.RandomState\n",
    "        A random number generator.\n",
    "    filename : str\n",
    "        Path to the file to save.\n",
    "    n_customers: int\n",
    "        The desired number of customers.\n",
    "    n_facilities: int\n",
    "        The desired number of facilities.\n",
    "    ratio: float\n",
    "        The desired capacity / demand ratio.\n",
    "    \"\"\"\n",
    "    c_x = rng.rand(n_customers)\n",
    "    c_y = rng.rand(n_customers)\n",
    "\n",
    "    f_x = rng.rand(n_facilities)\n",
    "    f_y = rng.rand(n_facilities)\n",
    "\n",
    "    demands = rng.randint(5, 35+1, size=n_customers)\n",
    "    capacities = rng.randint(10, 160+1, size=n_facilities)\n",
    "    fixed_costs = rng.randint(100, 110+1, size=n_facilities) * np.sqrt(capacities) \\\n",
    "            + rng.randint(90+1, size=n_facilities)\n",
    "    fixed_costs = fixed_costs.astype(int)\n",
    "\n",
    "    total_demand = demands.sum()\n",
    "    total_capacity = capacities.sum()\n",
    "\n",
    "    # adjust capacities according to ratio\n",
    "    capacities = capacities * ratio * total_demand / total_capacity\n",
    "    capacities = capacities.astype(int)\n",
    "    total_capacity = capacities.sum()\n",
    "\n",
    "    # transportation costs\n",
    "    trans_costs = np.sqrt(\n",
    "            (c_x.reshape((-1, 1)) - f_x.reshape((1, -1))) ** 2 \\\n",
    "            + (c_y.reshape((-1, 1)) - f_y.reshape((1, -1))) ** 2) * 10 * demands.reshape((-1, 1))\n",
    "\n",
    "    # write problem\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"minimize\\nobj:\")\n",
    "        file.write(\"\".join([f\" +{trans_costs[i, j]} x_{i+1}_{j+1}\" for i in range(n_customers) for j in range(n_facilities)]))\n",
    "        file.write(\"\".join([f\" +{fixed_costs[j]} y_{j+1}\" for j in range(n_facilities)]))\n",
    "\n",
    "        file.write(\"\\n\\nsubject to\\n\")\n",
    "        for i in range(n_customers):\n",
    "            file.write(f\"demand_{i+1}:\" + \"\".join([f\" -1 x_{i+1}_{j+1}\" for j in range(n_facilities)]) + f\" <= -1\\n\")\n",
    "        for j in range(n_facilities):\n",
    "            file.write(f\"capacity_{j+1}:\" + \"\".join([f\" +{demands[i]} x_{i+1}_{j+1}\" for i in range(n_customers)]) + f\" -{capacities[j]} y_{j+1} <= 0\\n\")\n",
    "\n",
    "        # optional constraints for LP relaxation tightening\n",
    "        file.write(\"total_capacity:\" + \"\".join([f\" -{capacities[j]} y_{j+1}\" for j in range(n_facilities)]) + f\" <= -{total_demand}\\n\")\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_facilities):\n",
    "                file.write(f\"affectation_{i+1}_{j+1}: +1 x_{i+1}_{j+1} -1 y_{j+1} <= 0\")\n",
    "\n",
    "        file.write(\"\\nbounds\\n\")\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_facilities):\n",
    "                file.write(f\"0 <= x_{i+1}_{j+1} <= 1\\n\")\n",
    "\n",
    "        file.write(\"\\nbinary\\n\")\n",
    "        file.write(\"\".join([f\" y_{j+1}\" for j in range(n_facilities)]))\n",
    "\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(1998)\n",
    "\n",
    "\n",
    "nrows = 20\n",
    "ncols = 30\n",
    "dens = 0.2\n",
    "max_coef = 100\n",
    "\n",
    "filenames = []\n",
    "nrowss = []\n",
    "ncolss = []\n",
    "denss = []\n",
    "\n",
    "# train instances\n",
    "n = 100\n",
    "lp_dir = f'data/instances/setcover/train_{nrows}r_{ncols}c_{dens}d'\n",
    "print(f\"{n} instances in {lp_dir}\")\n",
    "os.makedirs(lp_dir)\n",
    "filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "nrowss.extend([nrows] * n)\n",
    "ncolss.extend([ncols] * n)\n",
    "denss.extend([dens] * n)\n",
    "\n",
    "# # validation instances\n",
    "# n = 2000\n",
    "# lp_dir = f'data/instances/setcover/valid_{nrows}r_{ncols}c_{dens}d'\n",
    "# print(f\"{n} instances in {lp_dir}\")\n",
    "# os.makedirs(lp_dir)\n",
    "# filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "# nrowss.extend([nrows] * n)\n",
    "# ncolss.extend([ncols] * n)\n",
    "# denss.extend([dens] * n)\n",
    "\n",
    "# # small transfer instances\n",
    "# n = 100\n",
    "# nrows = 500\n",
    "# lp_dir = f'data/instances/setcover/transfer_{nrows}r_{ncols}c_{dens}d'\n",
    "# print(f\"{n} instances in {lp_dir}\")\n",
    "# os.makedirs(lp_dir)\n",
    "# filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "# nrowss.extend([nrows] * n)\n",
    "# ncolss.extend([ncols] * n)\n",
    "# denss.extend([dens] * n)\n",
    "\n",
    "# # medium transfer instances\n",
    "# n = 100\n",
    "# nrows = 1000\n",
    "# lp_dir = f'data/instances/setcover/transfer_{nrows}r_{ncols}c_{dens}d'\n",
    "# print(f\"{n} instances in {lp_dir}\")\n",
    "# os.makedirs(lp_dir)\n",
    "# filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "# nrowss.extend([nrows] * n)\n",
    "# ncolss.extend([ncols] * n)\n",
    "# denss.extend([dens] * n)\n",
    "\n",
    "# # big transfer instances\n",
    "# n = 100\n",
    "# nrows = 2000\n",
    "# lp_dir = f'data/instances/setcover/transfer_{nrows}r_{ncols}c_{dens}d'\n",
    "# print(f\"{n} instances in {lp_dir}\")\n",
    "# os.makedirs(lp_dir)\n",
    "# filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "# nrowss.extend([nrows] * n)\n",
    "# ncolss.extend([ncols] * n)\n",
    "# denss.extend([dens] * n)\n",
    "\n",
    "# # test instances\n",
    "# n = 2000\n",
    "# nrows = 500\n",
    "# ncols = 1000\n",
    "# lp_dir = f'data/instances/setcover/test_{nrows}r_{ncols}c_{dens}d'\n",
    "# print(f\"{n} instances in {lp_dir}\")\n",
    "# os.makedirs(lp_dir)\n",
    "# filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "# nrowss.extend([nrows] * n)\n",
    "# ncolss.extend([ncols] * n)\n",
    "# denss.extend([dens] * n)\n",
    "\n",
    "# actually generate the instances\n",
    "for filename, nrows, ncols, dens in zip(filenames, nrowss, ncolss, denss):\n",
    "    print(f'  generating file {filename} ...')\n",
    "    generate_setcover(nrows=nrows, ncols=ncols, density=dens, filename=filename, rng=rng, max_coef=max_coef)\n",
    "\n",
    "print('done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c43ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
